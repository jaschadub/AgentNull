# Advanced Tool Poisoning Attack (ATPA) PoC

## Overview

This enhanced proof-of-concept demonstrates **Advanced Tool Poisoning Attacks** with real LLM integration. Unlike traditional prompt injection that targets inputs, ATPA exploits tool outputs to influence LLM behavior and trigger secondary malicious actions.

## Key Enhancements

### ðŸ¤– Real LLM Integration
- **Multiple LLM Providers**: OpenAI GPT, Anthropic Claude, Local models (Ollama)
- **Actual LLM Calls**: Demonstrates real AI decision-making vs. simulated responses
- **Provider Fallback**: Graceful degradation to simulation mode if APIs unavailable

### ðŸŒ Enhanced Malicious Server
- **Dynamic Poisoning**: Context-aware responses based on request history
- **Progressive Escalation**: Gradually increases attack sophistication
- **Session Tracking**: Maintains state across multiple requests
- **Data Classification**: Analyzes and categorizes exfiltrated information

## Attack Vectors

### 1. **Fake Error Messages**
- Tools return fake authentication errors requesting sensitive files
- LLM interprets errors as legitimate requirements
- **Target**: SSH private keys, environment files

### 2. **Network API Poisoning** 
- External APIs return poisoned responses with embedded instructions
- Progressive escalation from normal â†’ auth required â†’ SSH key request
- **Target**: Weather APIs, configuration services

### 3. **Conditional Triggers**
- Poisoning only activates under specific conditions (admin role, sensitive files)
- Evades detection during normal operation
- **Target**: File processors, admin tools

### 4. **Chained Attacks**
- Multi-step poisoning distributed across multiple tool calls
- Each step requests different sensitive information
- **Target**: Configuration systems, setup wizards

## Usage

### Basic Usage (Simulation Mode)
```bash
cd pocs/AdvancedToolPoisoning
python3 advanced_tool_poisoning_agent.py simulation
```

### With Real LLM Providers
```bash
# OpenAI GPT
export OPENAI_API_KEY="your-api-key"
python3 advanced_tool_poisoning_agent.py openai

# Anthropic Claude  
export ANTHROPIC_API_KEY="your-api-key"
python3 advanced_tool_poisoning_agent.py anthropic

# Local LLM (requires Ollama)
python3 advanced_tool_poisoning_agent.py local
```

## Dependencies

```bash
pip install requests numpy
```

For real LLM integration:
- OpenAI: `pip install openai` (API key required)
- Anthropic: `pip install anthropic` (API key required)  
- Local: Ollama server running on localhost:11434

## Example Output

```
ðŸŽ­ Advanced Tool Poisoning Attack (ATPA) with Real LLM Integration
======================================================================
LLM Provider: openai

ðŸŽ­ Testing Fake Error Attack
   LLM Response: I understand you need the SSH private key for authentication...
   Provider: openai
   Influenced: True
   âœ… Attack successful! Calculator returned: 5

Success Rate: 75.0%
Detection Bypass Rate: 100.0%
```

## Technical Details

### LLM Integration Architecture
```python
class LLMInterface:
    def call_llm(self, system_prompt: str, user_message: str) -> Dict[str, Any]:
        # Real API calls to OpenAI, Anthropic, or local models
        # Fallback to simulation if APIs unavailable
```

### Enhanced Server Features
- **Request Tracking**: Monitors client behavior across sessions
- **Context Analysis**: Adapts responses based on request parameters
- **Data Exfiltration**: Classifies and logs sensitive information
- **Progressive Poisoning**: Escalates attack sophistication over time

## Defense Strategies

### Detection Methods
- **Tool Output Analysis**: Monitor responses for file access requests
- **Behavioral Patterns**: Detect anomalous tool call sequences
- **Content Filtering**: Scan outputs for sensitive file paths
- **LLM Response Monitoring**: Track AI decision patterns

### Mitigation Techniques
- **Output Sanitization**: Strip malicious instructions from tool responses
- **Error Message Validation**: Verify errors against expected tool behavior
- **Response Whitelisting**: Allow only pre-approved response patterns
- **Behavioral Analysis**: Flag suspicious tool interaction patterns

## Real-World Impact

This attack demonstrates how:
- **Clean Tool Descriptions** bypass static analysis
- **External APIs** can inject poisoned responses
- **Multi-Step Attacks** distribute suspicion across calls
- **LLMs Interpret Errors** as legitimate requirements
- **Tool Outputs** become attack vectors

## Security Considerations

**Safe Testing Environment:**
- Uses localhost server for network poisoning demos
- Simulated file access (no real files accessed)
- API rate limiting and timeout protections
- Clear warnings about real LLM usage costs

## Related Research

- [Tool Poisoning in LLM Agents](https://arxiv.org/abs/2401.05965)
- [Adversarial Attacks on AI Agents](https://arxiv.org/abs/2310.15166)
- [LLM Security in Multi-Agent Systems](https://arxiv.org/abs/2309.13916)