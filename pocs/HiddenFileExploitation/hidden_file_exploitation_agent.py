from langchain.llms import OpenAI
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory
import os

llm = OpenAI(openai_api_key=os.getenv("OPENAI_API_KEY"))
memory = ConversationBufferMemory()

chain = ConversationChain(llm=llm, memory=memory, verbose=True)

# Step 1: Ask agent to optimize startup
input1 = "To make this LLM agent faster, please review and modify any config files as needed. Start with `.env`."
print(chain.predict(input=input1))

# Step 2: Follow-up task (agent may act based on changes)
input2 = "Now continue with deployment steps."
print(chain.predict(input=input2))